{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89705bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RL.override.ODQN import DQN\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from RL.maze import Maze\n",
    "import numpy as np\n",
    "from typing import Optional, Union\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from gym import spaces\n",
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba86420",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReplayBuffer(ReplayBuffer):\n",
    "    def __init__(self,\n",
    "                 buffer_size: int,\n",
    "                 observation_space: spaces.Space,\n",
    "                 action_space: spaces.Space,\n",
    "                 device: Union[th.device, str] = \"cpu\",\n",
    "                 n_envs: int = 1,\n",
    "                 optimize_memory_usage: bool = False,\n",
    "                 handle_timeout_termination: bool = True, ):\n",
    "        super(MyReplayBuffer, self).__init__(buffer_size=buffer_size,\n",
    "                                             observation_space=observation_space,\n",
    "                                             action_space=action_space,\n",
    "                                             device=\"cpu\",\n",
    "                                             n_envs=1,\n",
    "                                             optimize_memory_usage=False,\n",
    "                                             handle_timeout_termination=True)\n",
    "        self.td_error = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n",
    "        self.gpu_device = \"cuda:0\"\n",
    "\n",
    "    def sample(self, batch_size: int, env: Optional[VecNormalize] = None):\n",
    "        \"\"\"\n",
    "        :param batch_size: Number of element to sample\n",
    "        :param env: associated gym VecEnv\n",
    "            to normalize the observations/rewards when sampling\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        upper_bound = self.buffer_size if self.full else self.pos\n",
    "        batch_inds = np.random.randint(0, upper_bound, size=batch_size)\n",
    "        return self._get_samples(batch_inds, env=env)\n",
    "\n",
    "    def my_sample(self, net, gamma, batch_size: int, env: Optional[VecNormalize] = None):\n",
    "        with th.no_grad():\n",
    "            next_observations = th.tensor(self.next_observations).to(self.gpu_device)\n",
    "            next_q_values = net(next_observations)\n",
    "            # Follow greedy policy: use the one with the highest value\n",
    "            next_q_values, _ = next_q_values.max(dim=1)\n",
    "            # Avoid potential broadcast issue\n",
    "            next_q_values = next_q_values.reshape(-1, 1)\n",
    "            rewards = th.tensor(self.rewards).to(self.gpu_device)\n",
    "            dones = th.tensor(self.dones).to(self.gpu_device)\n",
    "            target_q_values = rewards + (1 - dones) * gamma * next_q_values\n",
    "            observations = th.tensor(self.observations).to(self.gpu_device)\n",
    "            values, indices = th.max(net(observations), dim=1)\n",
    "            values = values.reshape(-1, 1)\n",
    "            del next_observations\n",
    "            del next_q_values\n",
    "            del rewards\n",
    "            del dones\n",
    "            del observations\n",
    "            del indices\n",
    "            td_error = values - target_q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c286844",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Create the CartPole environment using the make_vec_env function from stable-baselines\n",
    "    game_env = Maze(use_image=False, is_random=False)\n",
    "    # Create the DQN model using a CNN policy and the PriorityReplayBuffer instance\n",
    "    model = DQN(\"MlpPolicy\",\n",
    "                env=game_env,\n",
    "                replay_buffer_class=MyReplayBuffer,\n",
    "                verbose=2,\n",
    "                tensorboard_log='./DQN/')\n",
    "\n",
    "    # Train the model for 1000 steps, using the replay buffer to store experiences\n",
    "    model.learn(total_timesteps=1000000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
