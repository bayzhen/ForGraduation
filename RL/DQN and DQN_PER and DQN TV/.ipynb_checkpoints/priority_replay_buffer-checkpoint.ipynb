{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326a5523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from typing import Any, Dict, Generator, List, Optional, Union\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.cuda\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env.vec_normalize import VecNormalize\n",
    "from stable_baselines3.dqn.policies import MlpPolicy\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from scipy.stats import norm\n",
    "from stable_baselines3.common.type_aliases import (\n",
    "    ReplayBufferSamples\n",
    ")\n",
    "from gym import spaces\n",
    "import torch as th\n",
    "from env_simulator import EnvSimulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e13612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriorityReplayBuffer(ReplayBuffer):\n",
    "    def __init__(\n",
    "            self,\n",
    "            buffer_size: int,\n",
    "            observation_space: spaces.Space,\n",
    "            action_space: spaces.Space,\n",
    "            device: Union[th.device, str] = \"cpu\",\n",
    "            n_envs: int = 1,\n",
    "            optimize_memory_usage: bool = False,\n",
    "            handle_timeout_termination: bool = True,\n",
    "    ):\n",
    "        super(PriorityReplayBuffer, self).__init__(buffer_size,\n",
    "                                                   observation_space,\n",
    "                                                   action_space,\n",
    "                                                   device,\n",
    "                                                   n_envs,\n",
    "                                                   optimize_memory_usage,\n",
    "                                                   handle_timeout_termination)\n",
    "        if type(observation_space) is gym.spaces.box.Box:\n",
    "            v0 = 1\n",
    "            for i in observation_space.shape:\n",
    "                v0 *= i\n",
    "            state_input_size = v0\n",
    "        if type(action_space) is gym.spaces.discrete.Discrete:\n",
    "            action_input_size = 1\n",
    "        self.env_simulator = EnvSimulator(state_input_size, action_input_size).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.env_simulator.parameters(), lr=0.001)\n",
    "\n",
    "    def sample(self, batch_size: int, env: Optional[VecNormalize] = None) -> ReplayBufferSamples:\n",
    "        rewards = np.expand_dims(self.rewards, axis=2)\n",
    "        if self.full:\n",
    "            nn_input = np.concatenate((self.observations, self.actions), axis=2)\n",
    "            nn_output = np.concatenate((rewards, self.next_observations), axis=2)\n",
    "        else:\n",
    "            nn_input = np.concatenate((self.observations, self.actions), axis=2)[:self.pos]\n",
    "            nn_output = np.concatenate((rewards, self.next_observations), axis=2)[:self.pos]\n",
    "        nn_input = nn_input.astype(np.float32)\n",
    "        nn_output = nn_output.astype(np.float32)\n",
    "        nn_input = torch.tensor(nn_input).to(self.device)\n",
    "        nn_output = torch.tensor(nn_output).to(self.device)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        loss = criterion(self.env_simulator(nn_input), nn_output)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        loss = loss.item()\n",
    "        print(f\"loss: {loss:>7f}\")\n",
    "        return super(PriorityReplayBuffer, self).sample(batch_size, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94540d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    # Create the CartPole environment using the make_vec_env function from stable-baselines\n",
    "    game_env = make_vec_env('CartPole-v1', n_envs=1)\n",
    "\n",
    "    # Create the DQN model using a CNN policy and the PriorityReplayBuffer instance\n",
    "    model = DQN(\"MlpPolicy\",\n",
    "                env=game_env,\n",
    "                replay_buffer_class=PriorityReplayBuffer,\n",
    "                verbose=1,\n",
    "                tensorboard_log='./DQN/')\n",
    "\n",
    "    # Train the model for 1000 steps, using the replay buffer to store experiences\n",
    "    model.learn(total_timesteps=1000000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
