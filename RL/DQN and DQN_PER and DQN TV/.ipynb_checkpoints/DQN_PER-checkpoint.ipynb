{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8b64c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Any, Dict, List, Optional, Tuple, Type, Union, NamedTuple\n",
    "import warnings\n",
    "import gym\n",
    "import gym.spaces as spaces\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import random\n",
    "\n",
    "# Select device\n",
    "if th.cuda.is_available():\n",
    "    device = th.device(\"cuda:0\")\n",
    "else:\n",
    "    device = th.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02e869c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.buffers import BaseBuffer\n",
    "from stable_baselines3.common.type_aliases import (\n",
    "    DictReplayBufferSamples,\n",
    "    DictRolloutBufferSamples,\n",
    "    ReplayBufferSamples,\n",
    "    RolloutBufferSamples,\n",
    ")\n",
    "try:\n",
    "    # Check memory used by replay buffer when possible\n",
    "    import psutil\n",
    "except ImportError:\n",
    "    psutil = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37e04f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBufferSamples(NamedTuple):\n",
    "    observations: th.Tensor\n",
    "    actions: th.Tensor\n",
    "    next_observations: th.Tensor\n",
    "    dones: th.Tensor\n",
    "    rewards: th.Tensor\n",
    "    weights: th.Tensor\n",
    "    indexed: th.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8684657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.utils import get_device\n",
    "from stable_baselines3.common.vec_env import VecNormalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbc0d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer(BaseBuffer):\n",
    "    \"\"\"\n",
    "    Replay buffer used in off-policy algorithms like SAC/TD3.\n",
    "\n",
    "    :param buffer_size: Max number of element in the buffer\n",
    "    :param observation_space: Observation space\n",
    "    :param action_space: Action space\n",
    "    :param device: PyTorch device\n",
    "    :param n_envs: Number of parallel environments\n",
    "    :param optimize_memory_usage: Enable a memory efficient variant\n",
    "        of the replay buffer which reduces by almost a factor two the memory used,\n",
    "        at a cost of more complexity.\n",
    "        See https://github.com/DLR-RM/stable-baselines3/issues/37#issuecomment-637501195\n",
    "        and https://github.com/DLR-RM/stable-baselines3/pull/28#issuecomment-637559274\n",
    "        Cannot be used in combination with handle_timeout_termination.\n",
    "    :param handle_timeout_termination: Handle timeout termination (due to timelimit)\n",
    "        separately and treat the task as infinite horizon task.\n",
    "        https://github.com/DLR-RM/stable-baselines3/issues/284\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        buffer_size: int,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Space,\n",
    "        device: Union[th.device, str] = \"auto\",\n",
    "        n_envs: int = 1,\n",
    "        optimize_memory_usage: bool = False,\n",
    "        handle_timeout_termination: bool = True,\n",
    "    ):\n",
    "        super().__init__(buffer_size, observation_space, action_space, device, n_envs=n_envs)\n",
    "\n",
    "        # Adjust buffer size\n",
    "        self.buffer_size = max(buffer_size // n_envs, 1)\n",
    "\n",
    "        # Check that the replay buffer can fit into the memory\n",
    "        if psutil is not None:\n",
    "            mem_available = psutil.virtual_memory().available\n",
    "\n",
    "        # there is a bug if both optimize_memory_usage and handle_timeout_termination are true\n",
    "        # see https://github.com/DLR-RM/stable-baselines3/issues/934\n",
    "        if optimize_memory_usage and handle_timeout_termination:\n",
    "            raise ValueError(\n",
    "                \"ReplayBuffer does not support optimize_memory_usage = True \"\n",
    "                \"and handle_timeout_termination = True simultaneously.\"\n",
    "            )\n",
    "        self.optimize_memory_usage = optimize_memory_usage\n",
    "        \n",
    "        # Markov add\n",
    "        # $\\alpha$\n",
    "        self.alpha = 0.6\n",
    "\n",
    "        # Maintain segment binary trees to take sum and find minimum over a range\n",
    "        self.priority_sum = [0 for _ in range(2 * self.buffer_size)]\n",
    "        self.priority_min = [float('inf') for _ in range(2 * self.buffer_size)]\n",
    "        \n",
    "        self.max_priority = 1\n",
    "        \n",
    "        self.observations = np.zeros((self.buffer_size, self.n_envs) + self.obs_shape, dtype=observation_space.dtype)\n",
    "\n",
    "        if optimize_memory_usage:\n",
    "            # `observations` contains also the next observation\n",
    "            self.next_observations = None\n",
    "        else:\n",
    "            self.next_observations = np.zeros((self.buffer_size, self.n_envs) + self.obs_shape, dtype=observation_space.dtype)\n",
    "\n",
    "        self.actions = np.zeros((self.buffer_size, self.n_envs, self.action_dim), dtype=action_space.dtype)\n",
    "\n",
    "        self.rewards = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n",
    "        self.dones = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n",
    "        # Handle timeouts termination properly if needed\n",
    "        # see https://github.com/DLR-RM/stable-baselines3/issues/284\n",
    "        self.handle_timeout_termination = handle_timeout_termination\n",
    "        self.timeouts = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n",
    "\n",
    "        if psutil is not None:\n",
    "            total_memory_usage = self.observations.nbytes + self.actions.nbytes + self.rewards.nbytes + self.dones.nbytes\n",
    "\n",
    "            if self.next_observations is not None:\n",
    "                total_memory_usage += self.next_observations.nbytes\n",
    "\n",
    "            if total_memory_usage > mem_available:\n",
    "                # Convert to GB\n",
    "                total_memory_usage /= 1e9\n",
    "                mem_available /= 1e9\n",
    "                warnings.warn(\n",
    "                    \"This system does not have apparently enough memory to store the complete \"\n",
    "                    f\"replay buffer {total_memory_usage:.2f}GB > {mem_available:.2f}GB\"\n",
    "                )\n",
    "\n",
    "    def add(\n",
    "        self,\n",
    "        obs: np.ndarray,\n",
    "        next_obs: np.ndarray,\n",
    "        action: np.ndarray,\n",
    "        reward: np.ndarray,\n",
    "        done: np.ndarray,\n",
    "        infos: List[Dict[str, Any]],\n",
    "    ) -> None:\n",
    "        #Markov add\n",
    "        idx = self.pos\n",
    "        \n",
    "        # Reshape needed when using multiple envs with discrete observations\n",
    "        # as numpy cannot broadcast (n_discrete,) to (n_discrete, 1)\n",
    "        if isinstance(self.observation_space, spaces.Discrete):\n",
    "            obs = obs.reshape((self.n_envs,) + self.obs_shape)\n",
    "            next_obs = next_obs.reshape((self.n_envs,) + self.obs_shape)\n",
    "\n",
    "        # Same, for actions\n",
    "        action = action.reshape((self.n_envs, self.action_dim))\n",
    "\n",
    "        # Copy to avoid modification by reference\n",
    "        self.observations[self.pos] = np.array(obs).copy()\n",
    "\n",
    "        if self.optimize_memory_usage:\n",
    "            self.observations[(self.pos + 1) % self.buffer_size] = np.array(next_obs).copy()\n",
    "        else:\n",
    "            self.next_observations[self.pos] = np.array(next_obs).copy()\n",
    "\n",
    "        self.actions[self.pos] = np.array(action).copy()\n",
    "        self.rewards[self.pos] = np.array(reward).copy()\n",
    "        self.dones[self.pos] = np.array(done).copy()\n",
    "\n",
    "        if self.handle_timeout_termination:\n",
    "            self.timeouts[self.pos] = np.array([info.get(\"TimeLimit.truncated\", False) for info in infos])\n",
    "\n",
    "        self.pos += 1\n",
    "        if self.pos == self.buffer_size:\n",
    "            self.full = True\n",
    "            self.pos = 0\n",
    "            \n",
    "        # $p_i^\\alpha$, new samples get `max_priority`\n",
    "        priority_alpha = self.max_priority ** self.alpha\n",
    "        # Update the two segment trees for sum and minimum\n",
    "        self._set_priority_min(idx, priority_alpha)\n",
    "        self._set_priority_sum(idx, priority_alpha)\n",
    "\n",
    "    def sample(self, batch_size: int, env: Optional[VecNormalize] = None) -> ReplayBufferSamples:\n",
    "        \"\"\"\n",
    "        ### Sample from buffer\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize samples\n",
    "        samples = {\n",
    "            'weights': np.zeros(shape=batch_size, dtype=np.float32),\n",
    "            'indexes': np.zeros(shape=batch_size, dtype=np.int32)\n",
    "        }\n",
    "\n",
    "        # Get sample indexes\n",
    "        for i in range(batch_size):\n",
    "            p = random.random() * self._sum()\n",
    "            idx = self.find_prefix_sum_idx(p)\n",
    "            samples['indexes'][i] = idx\n",
    "\n",
    "        # $\\min_i P(i) = \\frac{\\min_i p_i^\\alpha}{\\sum_k p_k^\\alpha}$\n",
    "        prob_min = self._min() / self._sum()\n",
    "        # $\\max_i w_i = \\bigg(\\frac{1}{N} \\frac{1}{\\min_i P(i)}\\bigg)^\\beta$\n",
    "        max_weight = (prob_min * self.size) ** (-beta)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            idx = samples['indexes'][i]\n",
    "            # $P(i) = \\frac{p_i^\\alpha}{\\sum_k p_k^\\alpha}$\n",
    "            prob = self.priority_sum[idx + self.buffer_size] / self._sum()\n",
    "            # $w_i = \\bigg(\\frac{1}{N} \\frac{1}{P(i)}\\bigg)^\\beta$\n",
    "            weight = (prob * self.size) ** (-beta)\n",
    "            # Normalize by $\\frac{1}{\\max_i w_i}$,\n",
    "            #  which also cancels off the $\\frac{1}{N}$ term\n",
    "            samples['weights'][i] = weight / max_weight\n",
    "        \n",
    "        batch_inds = samples['indexes']\n",
    "        \n",
    "        # Sample randomly the env idx\n",
    "        env_indices = np.random.randint(0, high=self.n_envs, size=(len(batch_inds),))\n",
    "\n",
    "        if self.optimize_memory_usage:\n",
    "            next_obs = self._normalize_obs(self.observations[(batch_inds + 1) % self.buffer_size, env_indices, :], env)\n",
    "        else:\n",
    "            next_obs = self._normalize_obs(self.next_observations[batch_inds, env_indices, :], env)\n",
    "\n",
    "        data = (\n",
    "            self._normalize_obs(self.observations[batch_inds, env_indices, :], env),\n",
    "            self.actions[batch_inds, env_indices, :],\n",
    "            next_obs,\n",
    "            # Only use dones that are not due to timeouts\n",
    "            # deactivated by default (timeouts is initialized as an array of False)\n",
    "            (self.dones[batch_inds, env_indices] * (1 - self.timeouts[batch_inds, env_indices])).reshape(-1, 1),\n",
    "            self._normalize_reward(self.rewards[batch_inds, env_indices].reshape(-1, 1), env),\n",
    "            samples['weights'],\n",
    "            samples['indexes']\n",
    "        )\n",
    "        return ReplayBufferSamples(*tuple(map(self.to_torch, data)))\n",
    "    \n",
    "    def _get_samples(\n",
    "        self, batch_inds: np.ndarray, env: Optional[VecNormalize] = None\n",
    "    ) -> Union[ReplayBufferSamples, RolloutBufferSamples]:\n",
    "        pass\n",
    "    \n",
    "    def _set_priority_min(self, idx, priority_alpha):\n",
    "        \"\"\"\n",
    "        #### Set priority in binary segment tree for minimum\n",
    "        \"\"\"\n",
    "\n",
    "        # Leaf of the binary tree\n",
    "        idx += self.buffer_size\n",
    "        self.priority_min[idx] = priority_alpha\n",
    "\n",
    "        # Update tree, by traversing along ancestors.\n",
    "        # Continue until the root of the tree.\n",
    "        while idx >= 2:\n",
    "            # Get the index of the parent node\n",
    "            idx //= 2\n",
    "            # Value of the parent node is the minimum of it's two children\n",
    "            self.priority_min[idx] = min(self.priority_min[2 * idx], self.priority_min[2 * idx + 1])\n",
    "\n",
    "    def _set_priority_sum(self, idx, priority):\n",
    "        \"\"\"\n",
    "        #### Set priority in binary segment tree for sum\n",
    "        \"\"\"\n",
    "\n",
    "        # Leaf of the binary tree\n",
    "        idx += self.buffer_size\n",
    "        # Set the priority at the leaf\n",
    "        self.priority_sum[idx] = priority\n",
    "\n",
    "        # Update tree, by traversing along ancestors.\n",
    "        # Continue until the root of the tree.\n",
    "        while idx >= 2:\n",
    "            # Get the index of the parent node\n",
    "            idx //= 2\n",
    "            # Value of the parent node is the sum of it's two children\n",
    "            self.priority_sum[idx] = self.priority_sum[2 * idx] + self.priority_sum[2 * idx + 1]\n",
    "\n",
    "    def _sum(self):\n",
    "        \"\"\"\n",
    "        #### $\\sum_k p_k^\\alpha$\n",
    "        \"\"\"\n",
    "\n",
    "        # The root node keeps the sum of all values\n",
    "        return self.priority_sum[1]\n",
    "\n",
    "    def _min(self):\n",
    "        \"\"\"\n",
    "        #### $\\min_k p_k^\\alpha$\n",
    "        \"\"\"\n",
    "\n",
    "        # The root node keeps the minimum of all values\n",
    "        return self.priority_min[1]\n",
    "\n",
    "    def find_prefix_sum_idx(self, prefix_sum):\n",
    "        \"\"\"\n",
    "        #### Find largest $i$ such that $\\sum_{k=1}^{i} p_k^\\alpha  \\le P$\n",
    "        \"\"\"\n",
    "\n",
    "        # Start from the root\n",
    "        idx = 1\n",
    "        while idx < self.buffer_size:\n",
    "            # If the sum of the left branch is higher than required sum\n",
    "            if self.priority_sum[idx * 2] > prefix_sum:\n",
    "                # Go to left branch of the tree\n",
    "                idx = 2 * idx\n",
    "            else:\n",
    "                # Otherwise go to right branch and reduce the sum of left\n",
    "                #  branch from required sum\n",
    "                prefix_sum -= self.priority_sum[idx * 2]\n",
    "                idx = 2 * idx + 1\n",
    "\n",
    "        # We are at the leaf node. Subtract the buffer_size by the index in the tree\n",
    "        # to get the index of actual value\n",
    "        return idx - self.buffer_size\n",
    "\n",
    "    def update_priorities(self, indexes, priorities):\n",
    "        \"\"\"\n",
    "        ### Update priorities\n",
    "        \"\"\"\n",
    "\n",
    "        for idx, priority in zip(indexes, priorities):\n",
    "            # Set current max priority\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "\n",
    "            # Calculate $p_i^\\alpha$\n",
    "            priority_alpha = priority ** self.alpha\n",
    "            # Update the trees\n",
    "            self._set_priority_min(idx, priority_alpha)\n",
    "            self._set_priority_sum(idx, priority_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a22e5eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e70add6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(nn.Module):\n",
    "    r\"\"\"\n",
    "    Wraps ``torch.nn.Module`` to overload ``__call__`` instead of\n",
    "    ``forward`` for better type checking.\n",
    "    \n",
    "    `PyTorch Github issue for clarification <https://github.com/pytorch/pytorch/issues/44605>`_\n",
    "    \"\"\"\n",
    "\n",
    "    def _forward_unimplemented(self, *input: Any) -> None:\n",
    "        # To stop PyTorch from giving abstract methods warning\n",
    "        pass\n",
    "\n",
    "    def __init_subclass__(cls, **kwargs):\n",
    "        if cls.__dict__.get('__call__', None) is None:\n",
    "            return\n",
    "\n",
    "        setattr(cls, 'forward', cls.__dict__['__call__'])\n",
    "        delattr(cls, '__call__')\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        params = self.parameters()\n",
    "        try:\n",
    "            sample_param = next(params)\n",
    "            return sample_param.device\n",
    "        except StopIteration:\n",
    "            raise RuntimeError(f\"Unable to determine\"\n",
    "                               f\" device of {self.__class__.__name__}\") from None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2beb612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Schedule:\n",
    "    def __call__(self, x):\n",
    "        raise NotImplementedError()\n",
    "class Piecewise(Schedule):\n",
    "    \"\"\"\n",
    "    ## Piecewise schedule\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, endpoints: List[Tuple[float, float]], outside_value: float = None):\n",
    "        \"\"\"\n",
    "        ### Initialize\n",
    "\n",
    "        `endpoints` is list of pairs `(x, y)`.\n",
    "         The values between endpoints are linearly interpolated.\n",
    "        `y` values outside the range covered by `x` are\n",
    "        `outside_value`.\n",
    "        \"\"\"\n",
    "\n",
    "        # `(x, y)` pairs should be sorted\n",
    "        indexes = [e[0] for e in endpoints]\n",
    "        assert indexes == sorted(indexes)\n",
    "\n",
    "        self._outside_value = outside_value\n",
    "        self._endpoints = endpoints\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        ### Find `y` for given `x`\n",
    "        \"\"\"\n",
    "\n",
    "        # iterate through each segment\n",
    "        for (x1, y1), (x2, y2) in zip(self._endpoints[:-1], self._endpoints[1:]):\n",
    "            # interpolate if `x` is within the segment\n",
    "            if x1 <= x < x2:\n",
    "                dx = float(x - x1) / (x2 - x1)\n",
    "                return y1 + dx * (y2 - y1)\n",
    "\n",
    "        # return outside value otherwise\n",
    "        return self._outside_value\n",
    "\n",
    "    def __str__(self):\n",
    "        endpoints = \", \".join([f\"({e[0]}, {e[1]})\" for e in self._endpoints])\n",
    "        return f\"Schedule[{endpoints}, {self._outside_value}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c3c5e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QFuncLoss(Module):\n",
    "\n",
    "    def __init__(self, gamma: float):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.huber_loss = nn.SmoothL1Loss(reduction='none')\n",
    "\n",
    "    def forward(self, q: th.Tensor, action: th.Tensor, double_q: th.Tensor,\n",
    "                target_q: th.Tensor, done: th.Tensor, reward: th.Tensor,\n",
    "                weights: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n",
    "        q_sampled_action = q.gather(-1, action.to(th.long).unsqueeze(-1)).squeeze(-1)\n",
    "        with th.no_grad():\n",
    "            best_next_action = th.argmax(double_q, -1)\n",
    "            best_next_q_value = target_q.gather(-1, best_next_action.unsqueeze(-1)).squeeze(-1)\n",
    "            q_update = reward + self.gamma * best_next_q_value * (1 - done)\n",
    "            td_error = q_sampled_action - q_update\n",
    "        losses = self.huber_loss(q_sampled_action, q_update)\n",
    "        loss = th.mean(weights * losses)\n",
    "        return td_error, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a690f0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from stable_baselines3.common.off_policy_algorithm import OffPolicyAlgorithm\n",
    "from stable_baselines3.common.preprocessing import maybe_transpose\n",
    "from stable_baselines3.common.policies import BasePolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import get_linear_fn, is_vectorized_observation, polyak_update\n",
    "from stable_baselines3.dqn.policies import CnnPolicy, DQNPolicy, MlpPolicy, MultiInputPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf7a3b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_PER(OffPolicyAlgorithm):\n",
    "    policy_aliases: Dict[str, Type[BasePolicy]] = {\n",
    "        \"MlpPolicy\": MlpPolicy,\n",
    "        \"CnnPolicy\": CnnPolicy,\n",
    "        \"MultiInputPolicy\": MultiInputPolicy,\n",
    "    }\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy: Union[str, Type[DQNPolicy]],\n",
    "        env: Union[GymEnv, str],\n",
    "        learning_rate: Union[float, Schedule] = 1e-4,\n",
    "        buffer_size: int = 1_000_000,  # 1e6\n",
    "        learning_starts: int = 50000,\n",
    "        batch_size: int = 32,\n",
    "        tau: float = 1.0,\n",
    "        gamma: float = 0.99,\n",
    "        train_freq: Union[int, Tuple[int, str]] = 4,\n",
    "        gradient_steps: int = 1,\n",
    "        replay_buffer_class: Optional[Type[ReplayBuffer]] = None,\n",
    "        replay_buffer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        optimize_memory_usage: bool = False,\n",
    "        target_update_interval: int = 10000,\n",
    "        exploration_fraction: float = 0.1,\n",
    "        exploration_initial_eps: float = 1.0,\n",
    "        exploration_final_eps: float = 0.05,\n",
    "        max_grad_norm: float = 10,\n",
    "        tensorboard_log: Optional[str] = None,\n",
    "        policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        verbose: int = 0,\n",
    "        seed: Optional[int] = None,\n",
    "        device: Union[th.device, str] = \"auto\",\n",
    "        _init_setup_model: bool = True,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            policy,\n",
    "            env,\n",
    "            learning_rate,\n",
    "            buffer_size,\n",
    "            learning_starts,\n",
    "            batch_size,\n",
    "            tau,\n",
    "            gamma,\n",
    "            train_freq,\n",
    "            gradient_steps,\n",
    "            action_noise=None,  # No action noise\n",
    "            replay_buffer_class=replay_buffer_class,\n",
    "            replay_buffer_kwargs=replay_buffer_kwargs,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            verbose=verbose,\n",
    "            device=device,\n",
    "            seed=seed,\n",
    "            sde_support=False,\n",
    "            optimize_memory_usage=optimize_memory_usage,\n",
    "            supported_action_spaces=(spaces.Discrete,),\n",
    "            support_multi_env=True,\n",
    "        )\n",
    "        \n",
    "        # Markov add\n",
    "        self.loss_func = QFuncLoss(0.99)        \n",
    "        self.exploration_initial_eps = exploration_initial_eps\n",
    "        self.exploration_final_eps = exploration_final_eps\n",
    "        self.exploration_fraction = exploration_fraction\n",
    "        self.target_update_interval = target_update_interval\n",
    "        # For updating the target network with multiple envs:\n",
    "        self._n_calls = 0\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        # \"epsilon\" for the epsilon-greedy exploration\n",
    "        self.exploration_rate = 0.0\n",
    "        # Linear schedule will be defined in `_setup_model()`\n",
    "        self.exploration_schedule = None\n",
    "        self.q_net, self.q_net_target = None, None\n",
    "\n",
    "        if _init_setup_model:\n",
    "            self._setup_model()\n",
    "\n",
    "    def _setup_model(self) -> None:\n",
    "        super(DQN_PER, self)._setup_model()\n",
    "        self._create_aliases()\n",
    "        self.exploration_schedule = get_linear_fn(\n",
    "            self.exploration_initial_eps,\n",
    "            self.exploration_final_eps,\n",
    "            self.exploration_fraction,\n",
    "        )\n",
    "        # Account for multiple environments\n",
    "        # each call to step() corresponds to n_envs transitions\n",
    "        if self.n_envs > 1:\n",
    "            if self.n_envs > self.target_update_interval:\n",
    "                warnings.warn(\n",
    "                    \"The number of environments used is greater than the target network \"\n",
    "                    f\"update interval ({self.n_envs} > {self.target_update_interval}), \"\n",
    "                    \"therefore the target network will be updated after each call to env.step() \"\n",
    "                    f\"which corresponds to {self.n_envs} steps.\"\n",
    "                )\n",
    "\n",
    "            self.target_update_interval = max(self.target_update_interval // self.n_envs, 1)\n",
    "\n",
    "    def _create_aliases(self) -> None:\n",
    "        self.q_net = self.policy.q_net\n",
    "        self.q_net_target = self.policy.q_net_target\n",
    "\n",
    "    def _on_step(self) -> None:\n",
    "        \"\"\"\n",
    "        Update the exploration rate and target network if needed.\n",
    "        This method is called in ``collect_rollouts()`` after each step in the environment.\n",
    "        \"\"\"\n",
    "        self._n_calls += 1\n",
    "        if self._n_calls % self.target_update_interval == 0:\n",
    "            polyak_update(self.q_net.parameters(), self.q_net_target.parameters(), self.tau)\n",
    "\n",
    "        self.exploration_rate = self.exploration_schedule(self._current_progress_remaining)\n",
    "        self.logger.record(\"rollout/exploration_rate\", self.exploration_rate)\n",
    "\n",
    "    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n",
    "        # Switch to train mode (this affects batch norm / dropout)\n",
    "        self.policy.set_training_mode(True)\n",
    "        # Update learning rate according to schedule\n",
    "        self._update_learning_rate(self.policy.optimizer)\n",
    "        losses = []\n",
    "        for _ in range(gradient_steps):\n",
    "            # Sample replay buffer\n",
    "            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n",
    "            with th.no_grad():\n",
    "                # Compute the next Q-values using the target network\n",
    "                next_q_values = self.q_net_target(replay_data.next_observations)\n",
    "                # Follow greedy policy: use the one with the highest value\n",
    "                next_q_values, _ = next_q_values.max(dim=1)\n",
    "                # Avoid potential broadcast issue\n",
    "                next_q_values = next_q_values.reshape(-1, 1)\n",
    "                # 1-step TD target\n",
    "                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n",
    "\n",
    "            # Get current Q-values estimates\n",
    "            current_q_values = self.q_net(replay_data.observations)\n",
    "            \n",
    "            # Compute Temporal Difference (TD) errors, $\\delta$, and the loss, $\\mathcal{L}(\\theta)$.\n",
    "            td_errors, loss = self.loss_func(current_q_values,\n",
    "                                             replay_data.actions,\n",
    "                                             next_q_values,\n",
    "                                             target_q_values,\n",
    "                                             replay_data.dones,\n",
    "                                             replay_data.rewards,\n",
    "                                             replay_data.weights)\n",
    "            \n",
    "            # Calculate priorities for replay buffer $p_i = |\\delta_i| + \\epsilon$\n",
    "            new_priorities = np.abs(td_errors.cpu().numpy()) + 1e-6\n",
    "            # Update replay buffer priorities\n",
    "            self.replay_buffer.update_priorities(replay_data.indexes, new_priorities)\n",
    "            \n",
    "            # Retrieve the q-values for the actions from the replay buffer\n",
    "            current_q_values = th.gather(current_q_values, dim=1, index=replay_data.actions.long())\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Optimize the policy\n",
    "            self.policy.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Clip gradient norm\n",
    "            th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "            self.policy.optimizer.step()\n",
    "\n",
    "        # Increase update counter\n",
    "        self._n_updates += gradient_steps\n",
    "\n",
    "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
    "        self.logger.record(\"train/loss\", np.mean(losses))\n",
    "    \n",
    "    def predict(\n",
    "            self,\n",
    "            observation: np.ndarray,\n",
    "            state: Optional[Tuple[np.ndarray, ...]] = None,\n",
    "            episode_start: Optional[np.ndarray] = None,\n",
    "            deterministic: bool = False,\n",
    "    ) -> Tuple[np.ndarray, Optional[Tuple[np.ndarray, ...]]]:\n",
    "        \"\"\"\n",
    "        Overrides the base_class predict function to include epsilon-greedy exploration.\n",
    "\n",
    "        :param observation: the input observation\n",
    "        :param state: The last states (can be None, used in recurrent policies)\n",
    "        :param episode_start: The last masks (can be None, used in recurrent policies)\n",
    "        :param deterministic: Whether or not to return deterministic actions.\n",
    "        :return: the model's action and the next state\n",
    "            (used in recurrent policies)\n",
    "        \"\"\"\n",
    "        if not deterministic and np.random.rand() < self.exploration_rate:\n",
    "            if is_vectorized_observation(maybe_transpose(observation, self.observation_space), self.observation_space):\n",
    "                if isinstance(self.observation_space, gym.spaces.Dict):\n",
    "                    n_batch = observation[list(observation.keys())[0]].shape[0]\n",
    "                else:\n",
    "                    n_batch = observation.shape[0]\n",
    "                action = np.array([self.action_space.sample() for _ in range(n_batch)])\n",
    "            else:\n",
    "                action = np.array(self.action_space.sample())\n",
    "        else:\n",
    "            action, state = self.policy.predict(observation, state, episode_start, deterministic)\n",
    "        return action, state\n",
    "\n",
    "\n",
    "    def learn(\n",
    "        self: SelfDQN,\n",
    "        total_timesteps: int,\n",
    "        callback: MaybeCallback = None,\n",
    "        log_interval: int = 4,\n",
    "        tb_log_name: str = \"DQN\",\n",
    "        reset_num_timesteps: bool = True,\n",
    "        progress_bar: bool = False,\n",
    "    ) -> SelfDQN:\n",
    "\n",
    "        return super().learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            callback=callback,\n",
    "            log_interval=log_interval,\n",
    "            tb_log_name=tb_log_name,\n",
    "            reset_num_timesteps=reset_num_timesteps,\n",
    "            progress_bar=progress_bar,\n",
    "        )\n",
    "\n",
    "    def _excluded_save_params(self) -> List[str]:\n",
    "        return super()._excluded_save_params() + [\"q_net\", \"q_net_target\"]\n",
    "\n",
    "    def _get_torch_save_params(self) -> Tuple[List[str], List[str]]:\n",
    "        state_dicts = [\"policy\", \"policy.optimizer\"]\n",
    "\n",
    "        return state_dicts, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "216cca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = [\"CartPole-v1\",\"MountainCar-v0\",\"LunarLander-v2\",\"Acrobot-v1\",\"Breakout-v0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50b832a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import _thread\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c28d6cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_of_envs = []\n",
    "def func(model, env):\n",
    "    state_of_envs.append(1)\n",
    "    model.learn(15000)\n",
    "    state_of_envs.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad6658c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.dqn import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "364d846e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in thread started by: <function func at 0x000002141D77D360>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21996\\3625664434.py\", line 4, in func\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21996\\410021500.py\", line 211, in learn\n",
      "TypeError: OffPolicyAlgorithm.learn() got an unexpected keyword argument 'eval_env'\n",
      "Exception ignored in thread started by: <function func at 0x000002141D77D360>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21996\\3625664434.py\", line 4, in func\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"C:\\anaconda3\\envs\\sb3\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py\", line 265, in learn\n",
      "    return super().learn(\n",
      "  File \"C:\\anaconda3\\envs\\sb3\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py\", line 323, in learn\n",
      "    total_timesteps, callback = self._setup_learn(\n",
      "  File \"C:\\anaconda3\\envs\\sb3\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py\", line 305, in _setup_learn\n",
      "    return super()._setup_learn(\n",
      "  File \"C:\\anaconda3\\envs\\sb3\\lib\\site-packages\\stable_baselines3\\common\\base_class.py\", line 416, in _setup_learn\n",
      "    self._logger = utils.configure_logger(self.verbose, self.tensorboard_log, tb_log_name, reset_num_timesteps)\n",
      "  File \"C:\\anaconda3\\envs\\sb3\\lib\\site-packages\\stable_baselines3\\common\\utils.py\", line 198, in configure_logger\n",
      "Exception ignored in thread started by: <function func at 0x000002141D77D360>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21996\\3625664434.py\", line 4, in func\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21996\\410021500.py\", line 211, in learn\n",
      "TypeError: OffPolicyAlgorithm.learn() got an unexpected keyword argument 'eval_env'\n",
      "Exception ignored in thread started by: <function func at 0x000002141D77D360>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21996\\3625664434.py\", line 4, in func\n",
      "    raise ImportError(\"Trying to log data to tensorboard but tensorboard is not installed.\")\n",
      "ImportError: Trying to log data to tensorboard but tensorboard is not installed.\n",
      "  File \"C:\\anaconda3\\envs\\sb3\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py\", line 265, in learn\n",
      "    return super().learn(\n",
      "  File \"C:\\anaconda3\\envs\\sb3\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py\", line 323, in learn\n",
      "    total_timesteps, callback = self._setup_learn(\n",
      "  File \"C:\\anaconda3\\envs\\sb3\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py\", line 305, in _setup_learn\n",
      "    return super()._setup_learn(\n",
      "  File \"C:\\anaconda3\\envs\\sb3\\lib\\site-packages\\stable_baselines3\\common\\base_class.py\", line 416, in _setup_learn\n",
      "    self._logger = utils.configure_logger(self.verbose, self.tensorboard_log, tb_log_name, reset_num_timesteps)\n",
      "  File \"C:\\anaconda3\\envs\\sb3\\lib\\site-packages\\stable_baselines3\\common\\utils.py\", line 198, in configure_logger\n",
      "    raise ImportError(\"Trying to log data to tensorboard but tensorboard is not installed.\")\n",
      "ImportError: Trying to log data to tensorboard but tensorboard is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in thread started by: <function func at 0x000002141D77D360>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21996\\3625664434.py\", line 4, in func\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21996\\410021500.py\", line 211, in learn\n",
      "TypeError: OffPolicyAlgorithm.learn() got an unexpected keyword argument 'eval_env'\n",
      "Exception ignored in thread started by: <function func at 0x000002141D77D360>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21996\\3625664434.py\", line 4, in func\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"C:\\anaconda3\\envs\\sb3\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py\", line 265, in learn\n",
      "    return super().learn(\n",
      "  File \"C:\\anaconda3\\envs\\sb3\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py\", line 323, in learn\n",
      "    total_timesteps, callback = self._setup_learn(\n",
      "  File \"C:\\anaconda3\\envs\\sb3\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py\", line 305, in _setup_learn\n",
      "    return super()._setup_learn(\n",
      "  File \"C:\\anaconda3\\envs\\sb3\\lib\\site-packages\\stable_baselines3\\common\\base_class.py\", line 416, in _setup_learn\n",
      "    self._logger = utils.configure_logger(self.verbose, self.tensorboard_log, tb_log_name, reset_num_timesteps)\n",
      "  File \"C:\\anaconda3\\envs\\sb3\\lib\\site-packages\\stable_baselines3\\common\\utils.py\", line 198, in configure_logger\n",
      "    raise ImportError(\"Trying to log data to tensorboard but tensorboard is not installed.\")\n",
      "ImportError: Trying to log data to tensorboard but tensorboard is not installed.\n",
      "Exception ignored in thread started by: <function func at 0x000002141D77D360>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21996\\3625664434.py\", line 4, in func\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21996\\410021500.py\", line 211, in learn\n",
      "TypeError: OffPolicyAlgorithm.learn() got an unexpected keyword argument 'eval_env'\n",
      "Exception ignored in thread started by: <function func at 0x000002141D77D360>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21996\\3625664434.py\", line 4, in func\n",
      "  File \"C:\\anaconda3\\envs\\sb3\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py\", line 265, in learn\n",
      "    return super().learn(\n",
      "  File \"C:\\anaconda3\\envs\\sb3\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py\", line 323, in learn\n",
      "    total_timesteps, callback = self._setup_learn(\n",
      "  File \"C:\\anaconda3\\envs\\sb3\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py\", line 305, in _setup_learn\n",
      "    return super()._setup_learn(\n",
      "  File \"C:\\anaconda3\\envs\\sb3\\lib\\site-packages\\stable_baselines3\\common\\base_class.py\", line 416, in _setup_learn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    self._logger = utils.configure_logger(self.verbose, self.tensorboard_log, tb_log_name, reset_num_timesteps)\n",
      "  File \"C:\\anaconda3\\envs\\sb3\\lib\\site-packages\\stable_baselines3\\common\\utils.py\", line 198, in configure_logger\n",
      "    raise ImportError(\"Trying to log data to tensorboard but tensorboard is not installed.\")\n",
      "ImportError: Trying to log data to tensorboard but tensorboard is not installed.\n"
     ]
    },
    {
     "ename": "NameNotFound",
     "evalue": "The environment `Breakout` has been moved out of Gym to the package `ale-py`. Please install the package via `pip install ale-py`. You can instantiate the new namespaced environment as `ALE/Breakout`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameNotFound\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m envs:\n\u001b[1;32m----> 3\u001b[0m         DQN_PER_game_env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;66;03m# Create the DQN model using a CNN policy and the PriorityReplayBuffer instance\u001b[39;00m\n\u001b[0;32m      5\u001b[0m         DQN_PER_model \u001b[38;5;241m=\u001b[39m DQN_PER(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m                     env\u001b[38;5;241m=\u001b[39mDQN_PER_game_env,\n\u001b[0;32m      7\u001b[0m                     replay_buffer_class\u001b[38;5;241m=\u001b[39mPrioritizedReplayBuffer,\n\u001b[0;32m      8\u001b[0m                     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m      9\u001b[0m                     tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./DQN_PER/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39menv)\n",
      "File \u001b[1;32mC:\\anaconda3\\envs\\sb3\\lib\\site-packages\\gym\\envs\\registration.py:676\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, **kwargs)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake\u001b[39m(\u001b[38;5;28mid\u001b[39m: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnv\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m registry\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;28mid\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\anaconda3\\envs\\sb3\\lib\\site-packages\\gym\\envs\\registration.py:490\u001b[0m, in \u001b[0;36mEnvRegistry.make\u001b[1;34m(self, path, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m namespace, name, version \u001b[38;5;241m=\u001b[39m parse_env_id(path)\n\u001b[0;32m    489\u001b[0m \u001b[38;5;66;03m# Get all versions of this spec.\u001b[39;00m\n\u001b[1;32m--> 490\u001b[0m versions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_specs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;66;03m# We check what the latest version of the environment is and display\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# a warning if the user is attempting to initialize an older version\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;66;03m# or an unversioned one.\u001b[39;00m\n\u001b[0;32m    495\u001b[0m latest_versioned_spec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m spec: spec\u001b[38;5;241m.\u001b[39mversion, versions),\n\u001b[0;32m    497\u001b[0m     key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m spec: cast(\u001b[38;5;28mint\u001b[39m, spec\u001b[38;5;241m.\u001b[39mversion),\n\u001b[0;32m    498\u001b[0m     default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    499\u001b[0m )\n",
      "File \u001b[1;32mC:\\anaconda3\\envs\\sb3\\lib\\site-packages\\gym\\envs\\registration.py:220\u001b[0m, in \u001b[0;36mEnvSpecTree.versions\u001b[1;34m(self, namespace, name)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mversions\u001b[39m(\u001b[38;5;28mself\u001b[39m, namespace: Optional[\u001b[38;5;28mstr\u001b[39m], name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence[EnvSpec]:\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    Returns the versions associated with a namespace and name.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    See `gym/envs/__relocated__.py` for more info.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assert_name_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m     versions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree[namespace][name]\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m namespace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m internal_env_relocation_map:\n",
      "File \u001b[1;32mC:\\anaconda3\\envs\\sb3\\lib\\site-packages\\gym\\envs\\registration.py:297\u001b[0m, in \u001b[0;36mEnvSpecTree._assert_name_exists\u001b[1;34m(self, namespace, name)\u001b[0m\n\u001b[0;32m    295\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Did you mean: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestions[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;66;03m# Throw the error\u001b[39;00m\n\u001b[1;32m--> 297\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mNameNotFound(message)\n",
      "\u001b[1;31mNameNotFound\u001b[0m: The environment `Breakout` has been moved out of Gym to the package `ale-py`. Please install the package via `pip install ale-py`. You can instantiate the new namespaced environment as `ALE/Breakout`."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    for env in envs:\n",
    "        DQN_PER_game_env = gym.make(env)\n",
    "        # Create the DQN model using a CNN policy and the PriorityReplayBuffer instance\n",
    "        DQN_PER_model = DQN_PER(\"MlpPolicy\",\n",
    "                    env=DQN_PER_game_env,\n",
    "                    replay_buffer_class=PrioritizedReplayBuffer,\n",
    "                    verbose=2,\n",
    "                    tensorboard_log='./DQN_PER/'+env)\n",
    "        _thread.start_new_thread(func, (DQN_PER_model, env))\n",
    "        \n",
    "        DQN_game_env = gym.make(env)\n",
    "        DQN_model = DQN(\"MlpPolicy\",\n",
    "                    env=DQN_game_env,\n",
    "                    verbose=2,\n",
    "                    tensorboard_log='./DQN/'+env)\n",
    "        _thread.start_new_thread(func, (DQN_model, env))\n",
    "        \n",
    "    while len(state_of_envs):\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a959a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN_PER_game_env"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
